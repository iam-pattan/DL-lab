{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-6(Text_gen_RNN).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "B5RMzxnE7WGP",
        "YjtuJ-0A7Z90"
      ],
      "mount_file_id": "1o9nSJVEr3_1yohOpSzl-hybwRIx2bp9x",
      "authorship_tag": "ABX9TyPGg1n+QMVpqYQ03amaxUtM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iam-pattan/DL-lab/blob/main/Lab_6(Text_gen_RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RMzxnE7WGP"
      },
      "source": [
        "## Code-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07d-Zmvb7T4-"
      },
      "source": [
        "# This is more for fun than a puzzle\n",
        "# \n",
        "# Find some text, you could download a book from project gutenberg, or r you could dump \n",
        "# all of the code in this project into one text file with 'cat ../../**/*.py > code.txt'\n",
        "# \n",
        "# Next run this character-based GRU with char-gen.py some-text-file.txt\n",
        "# If you are on a GPU you should use CuDNNGRU in place of GRU\n",
        "# \n",
        "# See if you can get interesting results!  Play with the number of hidden nodes\n",
        "# and try other RNN structures.  Modifying the diversity number doesn't affect\n",
        "# the model but can lead to different output.\n",
        "#\n",
        "# This model loads all of the data into memory, and that will be huge (why?).\n",
        "# Another fun project would be to use fit_generator to process a larger dataset.\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM, SimpleRNN\n",
        "from keras.layers import CuDNNGRU, GRU\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gKuaKqM7g7x"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"text\", type=str)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "run = wandb.init()\n",
        "config = run.config\n",
        "config.hidden_nodes = 128\n",
        "config.batch_size = 256\n",
        "config.file = args.text\n",
        "config.maxlen = 200\n",
        "config.step = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaYH6dJb7lDb"
      },
      "source": [
        "# Only load first 100k charcters because we're not using memory efficiently\n",
        "text = io.open(config.file, encoding='utf-8').read()[:100000]\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SihwK6i7lH0"
      },
      "source": [
        "# build a sequence for every <config.step>-th character in the text\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - config.maxlen, config.step):\n",
        "    sentences.append(text[i: i + config.maxlen])\n",
        "    next_chars.append(text[i + config.maxlen])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pyqABwk7lL_"
      },
      "source": [
        "# build up one-hot encoded input x and output y where x is a character\n",
        "# in the text y is the next character in the text\n",
        "\n",
        "x = np.zeros((len(sentences), config.maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(config.hidden_nodes, input_shape=(config.maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwd4v9yA7q-a"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uav1m9L67rDp"
      },
      "source": [
        "class SampleText(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        start_index = random.randint(0, len(text) - config.maxlen - 1)\n",
        "\n",
        "        for diversity in [0.5, 1.2]:\n",
        "            print()\n",
        "            print('----- diversity:', diversity)\n",
        "\n",
        "            generated = ''\n",
        "            sentence = text[start_index: start_index + config.maxlen]\n",
        "            generated += sentence\n",
        "            print('----- Generating with seed: \"' + sentence + '\"')\n",
        "            sys.stdout.write(generated)\n",
        "\n",
        "            for i in range(200):\n",
        "                x_pred = np.zeros((1, config.maxlen, len(chars)))\n",
        "                for t, char in enumerate(sentence):\n",
        "                    x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "                preds = model.predict(x_pred, verbose=0)[0]\n",
        "                next_index = sample(preds, diversity)\n",
        "                next_char = indices_char[next_index]\n",
        "\n",
        "                generated += next_char\n",
        "                sentence = sentence[1:] + next_char\n",
        "\n",
        "                sys.stdout.write(next_char)\n",
        "                sys.stdout.flush()\n",
        "            print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFKtdZVt7rI5"
      },
      "source": [
        "model.fit(x, y, batch_size=config.batch_size,\n",
        "          epochs=100, callbacks=[SampleText(), WandbCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjtuJ-0A7Z90"
      },
      "source": [
        "## Code-2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5uQjIjQ-CYH"
      },
      "source": [
        "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhfx5D7z-VBX",
        "outputId": "8e424f03-98f4-46ac-b91e-feddd92c6661"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLHI4nna-VFS",
        "outputId": "b0c6ddf4-7a4f-4670-c0f8-6475303af0f2"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "raw_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(raw_text)} characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NEByh5q-VIF",
        "outputId": "b3b1348d-4174-4f06-9639-9432826b17a4"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(raw_text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVmDEcfPMi0s"
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV6itgH_MivX",
        "outputId": "c9d01198-73ba-4228-bfb6-0b84a9c4547d"
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  1115394\n",
            "Total Vocab:  39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbfiboKg-Vf8",
        "outputId": "75f72505-82ae-4876-94b2-dc88cecf6d0d"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  1115294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4J2dNxtMsHu"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua6NbW0E-VkI"
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pGzTMEb-Vnk"
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KZs1h4W-VrE",
        "outputId": "f8820e22-8901-4ef2-edcc-f7566ffe28fd"
      },
      "source": [
        "# fit the model\n",
        "model.fit(X[:100000], y[:100000], epochs=5, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 490s 625ms/step - loss: 3.1020\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.02751, saving model to weights-improvement-01-3.0275.hdf5\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 488s 624ms/step - loss: 2.8479\n",
            "\n",
            "Epoch 00002: loss improved from 3.02751 to 2.79993, saving model to weights-improvement-02-2.7999.hdf5\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 503s 643ms/step - loss: 2.7107\n",
            "\n",
            "Epoch 00003: loss improved from 2.79993 to 2.69333, saving model to weights-improvement-03-2.6933.hdf5\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 525s 671ms/step - loss: 2.6387\n",
            "\n",
            "Epoch 00004: loss improved from 2.69333 to 2.62835, saving model to weights-improvement-04-2.6283.hdf5\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 499s 638ms/step - loss: 2.5958\n",
            "\n",
            "Epoch 00005: loss improved from 2.62835 to 2.58072, saving model to weights-improvement-05-2.5807.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe4cc2b9f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_eXEMvz-Vuo"
      },
      "source": [
        "# load the network weights\n",
        "filename = '/content/weights-improvement-01-3.0275.hdf5'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdUY5TiL-VyR",
        "outputId": "0efa22e1-71f8-44c7-8c56-9ac36ca820b3"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" eard it;\n",
            "but, since it serves my purpose, i will venture\n",
            "to stale 't a little more.\n",
            "\n",
            "first citizen:\n",
            " \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqB7cs7y-V2d",
        "outputId": "b5b29437-922c-4a5a-915d-68d5e0a418f2"
      },
      "source": [
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to toe to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUF-J0txRd6Q"
      },
      "source": [
        "!cp '/content/weights-improvement-01-3.0275.hdf5' '/content/drive/MyDrive/Colab Notebooks'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}